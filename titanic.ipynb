{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\nimport os\nprint(os.listdir(\"../input\"))\n\nimport warnings\nwarnings.filterwarnings('ignore')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fa512b10206ab4a4799795b2e8c1f31dcc8d2358"
      },
      "cell_type": "markdown",
      "source": "## Brief Introduction\nFirst, the 2 datasets are imported. The training one contains survival value while the testing one does not.  <br>\nThe aim here is to predict the survival of passengers in the test dataset using the features given."
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "scrolled": false
      },
      "cell_type": "code",
      "source": "# Read datasets from csv\ndf_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')\n\n# Merge the 2 dataframes for EDA and feature engineeraing\nfull = pd.concat([df_train, df_test], axis = 0, sort=True)\n\n# Set PassengerId as Index\nfull.set_index('PassengerId', drop = False, inplace=True)\ntrain = full[:891]\n\n# Display Data\ndisplay(full.head(3))\nprint(f\"Dataset contains {full.shape[0]} records, with {full.shape[1]} variables.\")\nprint(f\"Variables:{list(full.columns)}\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f103e7ef33077f5a79dbeb16605876af937bbab5"
      },
      "cell_type": "markdown",
      "source": "## Missing Values\nMissing Values are found on Age, Cabin and Fare. <br>\nAs too many values are missing in Cabin, this features may not be useful for predicting survival;\nAge can be an important factor and could be inferred from other features, e.g., Title, Parch and the families the passengers was belong to."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f16c4cf2d6f429cd42e30debb0dff655225b1f58"
      },
      "cell_type": "code",
      "source": "# Identify Missing Values\ndisplay(full.isnull().sum())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e162bd25360654c38f2dcf44133265f79d0cff4a"
      },
      "cell_type": "markdown",
      "source": "## Exploring the Data - Distributions"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "554c52adb25ddaa3f656f3328489ba474ff3e98d",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "# Descriptive Statistics\nfull.describe(include=\"all\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3a36f4bf5c172fbc786504bd39cec0578e37a5f3",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "# EDA - Distributions\ncategorical_var = ['Pclass','Sex','SibSp','Parch','Embarked', 'Survived']\ncontinuous_var = ['Age','Fare']\n\n# Plot Categorical Var\nfig, axs = plt.subplots(4,3, figsize = (15,12))\nfor i,key in enumerate(categorical_var):\n     sns.countplot(data = full, x = key, ax = axs[i//3,i%3], color='teal')\n\n# Plot Age\nplt.subplot2grid((4,3),(2,0),rowspan=1,colspan=3);\nplt.hist(full.Age[full.Age.isna()!=True], bins=range(0,80,1), color='slategrey' );\nplt.xlabel('Age');\n\n# Plot Fare\nplt.subplot2grid((4,3),(3,0),rowspan=1,colspan=3);\nplt.hist(full.Fare[full.Fare.isna()!=True], bins=100, color='slategrey');\nplt.xlabel('Fare');\n\nprint(f\"survived: {full.Survived.mean()*100:.2f}%\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1d9dbf55133389fc7dc846ff25ef7ef02f872969"
      },
      "cell_type": "markdown",
      "source": "## EDA - Relationships between features and survival"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "d3e5b47fd3cc9baff7dbef21c33f595c10b1ff88"
      },
      "cell_type": "code",
      "source": "\n# Plot all categorical features with Survival rate\nf, axs = plt.subplots(3,5, sharey=True, figsize=(18,9))\nfor i,key in enumerate(categorical_var[:-1]): # except feature Survived\n    sns.barplot(data = full, x= key, y='Survived', ax = axs[i%3, i//3], color='teal');\n    axs[i%3, i//3].axhline(y=0.3838, color='k', linestyle='--')\n\n# Plot Correlation\ncorr = full.corr()\nplt.subplot2grid((3,5),(0,2),rowspan=3,colspan=3);\ncmap =sns.diverging_palette( 220 , 10 , as_cmap = True )\nsns.heatmap(corr, cmap = cmap,square=True, cbar_kws={ 'shrink' : .9 }, annot = True, annot_kws = { 'fontsize' : 12 });\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5a3c3a197737d839dc2f91d7a9f554b7aa9765ba"
      },
      "cell_type": "markdown",
      "source": "Sex seems to have a strong predictive power, which makes sense due to the \"Women and Children First\" instructions for deciding who can get on the lifeboats. <br>\nPclass and Fare also seem significant. These higher class passengers lives and have most of their activities near the deck, thus, closer to the lifeboats. <br>\nIt is surprising to find no correlation between Age and Survived. Their relationship may not be linear."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "ab30b1eda98fe02aa1a753f2f8916fa7f5509564"
      },
      "cell_type": "code",
      "source": "# Plot number of survived passengers by PClass, Sex and Age\nfacet = sns.FacetGrid(full, row = 'Pclass',col='Sex', hue = 'Survived', aspect=2)\nfacet.map(plt.hist, 'Age', histtype='step', bins = np.arange(0,80,4))\n\nfacet.add_legend();",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "23c3a50b9b55c6bb337e98e950ca3a7f55ba02c8"
      },
      "cell_type": "markdown",
      "source": "Clearly shown the \"Women first\" pattern. <br>\nChild survival advantage seems to apply for those < 12 years old. <br>\nMuch higher survival rate for people in 1st and 2nd class. Children and Women in these 2 classes have a much higher survival rate (some age range even with  100%), compared to those in the 3rd class (which has around 50% chance)"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "d8dd41b4da4fd631a36bbfc21d0c7c80e02034da"
      },
      "cell_type": "code",
      "source": "# Create Age Quartiles\nfull['Age_quartile'] = pd.qcut(full.Age,10)\n\n# Plot age quartiles by sex with survival rate\nplt.figure(figsize = (10,5))\nsns.barplot(data = full, x= 'Age_quartile', y='Survived', hue = 'Sex');\nplt.axhline(y=0.3838, color='k', linestyle='--')\nplt.xticks(rotation = 30);\nplt.title('Across All Classes');\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "472e0f95ea9f8349a3e1e84b9a694a38370fff23"
      },
      "cell_type": "code",
      "source": "full['Child'] = (full['Age'] <= 14).astype(int)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "89504e2006eaa5edfbd9e1d5658e06819ab00f9c"
      },
      "cell_type": "markdown",
      "source": "Age Advantage for boys with age <14. <br>\nIt may exist for fathers as well (trough in males with age 14-25, which were unlikely to have kids )."
    },
    {
      "metadata": {
        "_uuid": "b391071fe359f9b37a791f57a9f8d3c240fb28d1"
      },
      "cell_type": "markdown",
      "source": "## Feature Engineering\nOther interesting relationships to look at is between Survival, Parch and SibSp. It is not difficult to imagine those within the same family/ same group will stay together when in danger, thus, having any of them survived would mean the other members of the group will likely to have a better chance to survive, and vice versa."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "759350d828967d136cac8925e4fe3204b840e123"
      },
      "cell_type": "code",
      "source": "import re \n\n# function to parse surname of the passengers\ndef parse_surname(s):\n    m = re.search('(\\w+),.*',s)\n    return m.group(1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9a1c8fea5eda3689dd92a2eb02c8f5bc0141316f"
      },
      "cell_type": "markdown",
      "source": "### Identifing the Families by Surname\nFirst, parse the Surnames of the passengers. Those from the same family should share the surname. <br>\nSurnames are grouped together and their occurance caluculated respectively."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "637a0fdc9535fdfc43b6ffacc5efabd741dc1c49"
      },
      "cell_type": "code",
      "source": "family = pd.DataFrame()\n\n# Parse Surname from Name\nfamily['Surname'] = full.Name.map(parse_surname)\n\n# Assign codes to surname for later grouping\nsurname_count_dict = {}\nsurname_code_dict = {}\nfor i, name in enumerate(family['Surname'].unique()):\n    surname_count_dict[name] = sum(family['Surname']==name)\n    surname_code_dict[name] = i\n\nfamily['SurnameCode'] = family['Surname'].map(surname_code_dict)\nfamily['SurnameSize'] = family['Surname'].map(surname_count_dict)\nfamily['FamilySize'] = 1 + full.Parch + full.SibSp # True Family Size \n\n# Examples with common surname\ndisplay(full[family.Surname == 'Smith'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "62df9c9d4a5cdf8536b498d28a62662300bd3aff"
      },
      "cell_type": "markdown",
      "source": "However, some common surnames may be shared by people from different families. This is addressed by the following function. <br>\nTo judge if passengers are likely to be in the same family, the function check their ticket code.  <br>\nThe function decides if people with the same surname are from the same family by checking the level of  similarity of their tickets. <br>Those with the exact same tickets or tickets that have values close to each other are grouped together."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "782101a2709ddc00f0acf4aa832970401a728e6b"
      },
      "cell_type": "code",
      "source": "def tick2fam_gen(df):\n    \"\"\"\n    Function to judge if passengers are likely to be in the same family.\n    Input: DataFrame with Passenger surname and ticket\n    Return: Code generated to specify different families\n    \"\"\"\n    # initialize ticket dict\n    dict_tick2fam = {'000000': 0}\n    fam_counter = 0\n        \n    for i in df.index:    \n        keys = list(dict_tick2fam.keys())\n        chk_key = df.loc[i, 'Ticket']\n        for key in keys:\n            if len(chk_key) == len(key): #if their tickets have high similarity\n                if (chk_key[-4].isdigit()) & (key[-4].isdigit()): \n                    if (chk_key[:-2] == key[:-2]) & (np.abs(int(chk_key[-2:]) - int(key[-2:])) <= 10):\n                        dict_tick2fam[chk_key] = dict_tick2fam[key]\n                        break\n                    \n            if key == keys[-1]:\n                fam_counter += 1\n                dict_tick2fam[chk_key] = str(fam_counter)  \n                \n    return dict_tick2fam",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "da0dce563a31ac07ff2ada96dfd3ee710391c5c1"
      },
      "cell_type": "code",
      "source": "# DF to provide a view for checking if function works properly\nfamily_infer = pd.concat([family, full[['Parch','SibSp','Age','Name','Pclass','Ticket','Embarked','Survived']]], axis = 1)\n\n# Single out Surnames with size > true family size (may have more than 1 family involved)\nchk_surname = family_infer[family['FamilySize'] < family['SurnameSize']].Surname.unique() # surnames to check\n# chk_surname2 = family_infer[family['FamilySize'] > family['SurnameSize']].Surname.unique() # unidentified fam\n\n# Regrouping Families according to Family Size and Ticket.\nfamily['SurnameAdj'] = family['Surname'] #new column for corrected family_group\n\nfor s in chk_surname:\n    family_regroup = family_infer[family.Surname == s] #get family with specific surname\n    fam_code_dict = tick2fam_gen(family_regroup) #pass in df to get family codes within the same surname\n\n    for idx in family_regroup.index: #assign family code 1by1\n        curr_ticket = full.loc[idx].Ticket\n\n        if family_regroup.loc[idx].FamilySize == 1: #for passengers traveling alone\n            if family_regroup.Ticket.value_counts()[curr_ticket] > 1: #relatives that shares surname and ticket, which Parch and SibSp failed to record\n                family.loc[idx, 'SurnameAdj'] =  family.loc[idx].Surname + '-hidfam' + fam_code_dict[curr_ticket]\n            else: #single traveler\n                family.loc[idx, 'SurnameAdj'] =  family.loc[idx].Surname + '-single' + fam_code_dict[curr_ticket]\n        else: #families\n            family.loc[idx, 'SurnameAdj'] =  family.loc[idx].Surname + '-fam' + fam_code_dict[curr_ticket]\n\ndisplay(family[family.Surname == 'Smith'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c756c44a5b9bf63d37883e683a6cb407e55ff742"
      },
      "cell_type": "markdown",
      "source": "After Adjusting the surnames of families, group these true families together again. The no. of families here should increase."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c2c80d14e52bda1e0458ccf41a99a67555d6187a"
      },
      "cell_type": "code",
      "source": "# Assign codes to families\nFamily_count_dict = {}\nFamily_code_dict = {}\nfor i, name in enumerate(family['SurnameAdj'].unique()):\n    surname_count_dict[name] = sum(family['SurnameAdj']==name) # count no. of member of the same fam\n    surname_code_dict[name] = i # fam code\n\nfamily['FamilyCode'] = family['SurnameAdj'].map(surname_code_dict)\nfamily['FamilySize'] = family['SurnameAdj'].map(surname_count_dict)\n\nprint(f\"No. of Family Before Regrouping: {len(family.SurnameCode.unique())}\")\nprint(f\"No. of Family After Regrouping: {len(family.FamilyCode.unique())}\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c188cc03a6cfa68ba3b32e0654acf9b56b18d1f3"
      },
      "cell_type": "markdown",
      "source": "### Identify Roomates by Ticket\nPeople who share the same ticket can be families as well as friends traveling together. They are expected to stay together during the incidents. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "64e112d645fb22a21cae6f4ab28c63a2249447ce",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "# Identify Groups (Those holding the same ticket code, could be friends/family)\ngroup = pd.DataFrame(family[['FamilyCode','FamilySize']])\n\nticket_count = {}\nticket_code = {}\nfor i,ticket in enumerate(full.Ticket.unique()):\n    ticket_count[ticket] = sum(full.Ticket == ticket)\n    ticket_code[ticket] = i\n\ngroup['Ticket_code'] = full.Ticket.map(ticket_code)\ngroup['Ticket_size'] = full.Ticket.map(ticket_count)\n\nprint(f\"No. of Tickets Identified: {len(group['Ticket_code'].unique())}\")\ndisplay(full[(full.Ticket == 'A/4 48871') |(full.Ticket == 'A/4 48873')])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "784eda64614a50ad5aa9b14d019557b207bb510b"
      },
      "cell_type": "markdown",
      "source": "### Combining Friends and Families as Groups\nFinally, the families and friend groups are combined together.  <br>\nPeople who share either the same room or same family are grouped together."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e3f4520f65551ea20fc043f1bf095159cd05b2ee"
      },
      "cell_type": "code",
      "source": "def ChainCombineGroups(df, colA, colB):\n    '''\n    This function takes in 2 columns of labels and chain all items which share\n    the same labels within each of the 2 columns\n    input:\n    df - DataFrame\n    colA - Key for Col\n    colB - Key for Col  \n    output:\n    array of numeric grouping labels\n    '''\n    # make a copy of DFs for iteration\n    data = df.copy()\n    search_df = data.copy()\n    \n    group_count = 0\n\n    while not search_df.empty:\n\n        # Initiate pool and Select Reference item\n        pool = search_df.iloc[:1]\n        idx = pool.index\n\n        # Remove 1st item from searching df\n        search_df.drop(index = idx, inplace = True)\n\n        # Initialize Search\n        flag_init = 1\n        update = pd.DataFrame()\n\n        # While loop to exhausively search for commonalities, pool is updated until no more common features are found\n        while (flag_init or not update.empty):\n\n            flag_init = 0\n\n            # target labels to look for\n            pool_A_uniq = np.unique(pool[colA])\n            pool_B_uniq = np.unique(pool[colB])\n\n            for col in [colA,colB]:\n                idx = []\n\n                # get all indexs of items with the same label\n                for num in np.unique(pool[col]):\n                    idx.extend(search_df[search_df[col] == num].index)\n\n                # update pool\n                update = search_df.loc[idx]\n                pool = pd.concat([pool, update], axis = 0)\n\n                # remove item from searching df\n                search_df = search_df.drop(index = idx)\n\n            # assign group num\n            data.loc[pool.index, 'Group_'] = group_count\n\n        group_count += 1\n        \n    return np.array(data['Group_'].astype(int))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bf81ffee070583626c9b09d231f98cb3f8f8e265"
      },
      "cell_type": "code",
      "source": "# Assign Final group no.\ngroup['Group_code'] = ChainCombineGroups(group, 'FamilyCode', 'Ticket_code')\n         \nprint(f\"Family: {len(family.FamilyCode.unique())}\")\nprint(f\"Group: {len(group.Ticket_code.unique())}\")\nprint(f\"Combined: {len(group.Group_code.unique())}\")\ngroup.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "997fb141a1261032410c85e54edf4eef171d9c51"
      },
      "cell_type": "code",
      "source": "print('An example of grouping the both friends and family under a same group.')\ndisplay(pd.concat([full['Ticket'],family[['Surname','FamilyCode']],group[['Ticket_code','Group_code']]], axis = 1)[group['Group_code'] == 458])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "65dba7925fc034944ea32589d8fb6d24b7cc80b1"
      },
      "cell_type": "markdown",
      "source": "### Limitations:\nThe above function did fail to join some families back together, especially those who had different ticket numbers and had different surnames. <br> \nFor example, female siblings who were married and took different surnames; <br>\nand families who bought tickets with codes that has low similarity, which is likely to be found for those in the 1st Class. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6bab3325f530fd49657590204032c4c2e4372b7e"
      },
      "cell_type": "code",
      "source": "# Getting the Group Size\ngroup_count={}\ngroup_code = group['Group_code'].unique()\nfor code in group_code:\n    group_count[code] = sum(group.Group_code == code)\ngroup['Group_size'] = group.Group_code.map(group_count)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0431962860b148be85946adbf3f03fdafa2d9ef4"
      },
      "cell_type": "markdown",
      "source": "### Survival of the Group\nFinally, the thing that we wanted to know in the first place is if the members in their Family/Friends group has survived or not. Having a surviving friend/family member should have good predictive power of whether a passenger survived or not."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f44b3fa558cd495e13565397243757d6d896cfd7"
      },
      "cell_type": "code",
      "source": "# Prepare the df by adding the Survived features\ngroup_corr_test = pd.concat([group, full.Survived, family[['SurnameCode','SurnameSize']]], axis = 1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b71e42b2974eb853d764d7b376cc028a60970b8a"
      },
      "cell_type": "code",
      "source": "for param in [('SurnameCode','SurnameSize'),\n              ('FamilyCode','FamilySize'),\n              ('Ticket_code','Ticket_size'),\n              ('Group_code','Group_size')]: # keep group at last\n    \n    # No. of member survived in each group\n    n_member_survived_by_gp = group_corr_test.groupby(param[0]).Survived.sum()\n    \n    # No. of member survived in a particular group, discounting the passenger concerned\n    n_mem_survived = group_corr_test[param[0]].map(n_member_survived_by_gp)\n    n_mem_survived_adj = n_mem_survived - group_corr_test.Survived.apply(lambda x: 1 if x == 1 else 0)\n\n    # Same for the dead\n    n_member_dead_by_gp = group_corr_test.groupby(param[0]).Survived.count() - group_corr_test.groupby(param[0]).Survived.sum()\n    n_mem_dead  = group_corr_test[param[0]].map(n_member_dead_by_gp)\n    n_mem_dead_adj = n_mem_dead - group_corr_test.Survived.apply(lambda x: 1 if x == 0 else 0)\n\n    # How many people from that group that we do not have data on.\n    unknown_factor = (group_corr_test[param[1]] - n_mem_survived_adj - n_mem_dead_adj)/group_corr_test[param[1]]\n    confidence = 1 - unknown_factor\n\n    # Ratio of members survived in that group, ranging from -1 to 1, adjusted by the confidence weight\n    key = 'Confidence_member_survived'+'_'+param[0]\n    ratio = (1/group_corr_test[param[1]]) * (n_mem_survived_adj - n_mem_dead_adj)\n    group_corr_test[key] = confidence * ratio\n    \n    group['Ratio_member_survived'] = (1/group_corr_test[param[1]]) * (n_mem_survived_adj - n_mem_dead_adj)\n    group['Confidence_member_survived'] = confidence * ratio\n\n# Display Correlation\nplt.barh(group_corr_test.corr().Survived[-4:].index, group_corr_test.corr().Survived[-4:])\nplt.xlabel('Correlation with Survived');",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c79c47dc771704c4d535d6a94a1926ec2bd0bdc4"
      },
      "cell_type": "markdown",
      "source": "## Data Engineering - Simplifying the Ticket format\nTickets also provide information on where the passengers are located on ship, which may be vital for survival.\nHere, I group the tickets by their first few letters. Ticket heading with occurance < 10 are ignored."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "975c262ecd5c07338fc8a3e27f0b4fd7416fd010"
      },
      "cell_type": "code",
      "source": "def parse_ticket(str1):\n    m = re.search(r'(.*)(\\s\\d|\\s\\d{4,7}$)',str1)\n    s = re.search(r'[A-Z]+',str1)\n    if m:\n        str2 = m.group(1)\n        n =re.search(r'([A-Z]+)[^A-Z0-9]*([A-Z]+)*[^A-Z0-9]*([A-Z0-9]*)[^A-Z]*([A-Z]*)*',str2)\n        new_str = ''\n        if n:    \n            if n.group(1):\n                new_str+=n.group(1)\n                if n.group(2) or n.group(3):\n                    if n.group(2):\n                        new_str+=n.group(2)\n                    if n.group(3):\n                        new_str+=n.group(3)\n                        if n.group(4):\n                            new_str+=n.group(4)\n                            if n.group(5):\n                                new_str+=m.group(5)\n    elif s:\n        new_str = s.group(0)\n    else:\n        new_str = 'XXX'\n    return new_str",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "2a8dd0c9c41290d17424a62ac8d7d441e6a3c024"
      },
      "cell_type": "code",
      "source": "ticket = pd.DataFrame(full.Survived)\n\nticket['Ticket'] = full.Ticket.map(parse_ticket)\nd = dict(zip(ticket.Ticket.value_counts().index, ticket.Ticket.value_counts()))\nticket['Ticket_count'] = ticket['Ticket'].map(d)\nplt.figure(figsize = (12,6))\nsns.barplot(data = ticket[ticket['Ticket_count'] > 10], x = 'Ticket', y = 'Survived')\nplt.axhline(y=0.3838, color='k', linestyle='--');",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "29ba15a916f9aacca4fff77fdc6556af7823d40a"
      },
      "cell_type": "markdown",
      "source": "Tickets with the most Predictive power: A5, PC"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "3cf6476c405e6162acc68705ea0a26ed644bf30c"
      },
      "cell_type": "code",
      "source": "ticket['A5'] = (ticket['Ticket'] == 'A5').astype(int)\nticket['PC'] = (ticket['Ticket'] == 'PC').astype(int)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "53c1539c9a34a17263f89803db024188ea4f2788"
      },
      "cell_type": "markdown",
      "source": "## Adjustin Fare according to Ticket Size\nFare value  is found to be distorted as the Fare feature in original dataset calculates the total amount paid for one single ticket, i.e., no. of person * base rate of ticket. To get a more accurate fare paid by individual value, the fare is divided by the no. of person holding that ticket."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d3d22d6c3ad72976f5427737ff97c6139ee20665"
      },
      "cell_type": "code",
      "source": "# Fare Adjustment\nfull['Fare_adj'] = full.Fare/group.Ticket_size\n\n# Plot Fare Adjustment\nfig, axs = plt.subplots(2,figsize = (14,10))\naxs[0].hist(full.Fare[full.Fare.isna()!=True], bins=80);\naxs[0].set_title('Before Adjustment')\naxs[0].set_xlabel('Fare')\naxs[1].hist(full.Fare_adj[full.Fare_adj.isna()!=True], bins=80);\naxs[1].set_title('After Adjustment');\naxs[1].set_xlabel('Fare_adj');",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "17786dfacb252fe15925f62d506e0e482e0383bb"
      },
      "cell_type": "markdown",
      "source": "After adjustment, the 3 Pclass are more clearly shown by the 3 peaks of Adjusted Fare."
    },
    {
      "metadata": {
        "_uuid": "3decd2bd846f06ab4457f96adf0f7fdecfea6ede"
      },
      "cell_type": "markdown",
      "source": "## Handling Missing Values\n\n### Missing Fare\nAs mentioned before, there are missing values in Age, Fare, Embarked and Cabin. <br>\nHere, I dealt with the Age and Fare only."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "995a3020e5cd5e373f6196134f21ad8e562c990d"
      },
      "cell_type": "code",
      "source": "# Handle missing Fare\nfull[full.Fare.isnull()] #PassengerId 1044 has fare value missing\nfare_dict_nan = dict(full.groupby('Pclass').Fare_adj.mean())\nfull.loc[full.Fare.isnull(),['Fare','Fare_adj']]= fare_dict_nan[3] # fill value according to PClass",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1edfd6644c2e07aeeb62e8ef8088478922e551da"
      },
      "cell_type": "markdown",
      "source": "Missing Fare is filled with the mean value of the Fare of their respective PClass."
    },
    {
      "metadata": {
        "_uuid": "d4c080f849c3563df24fe64e48d15bfebc47b434"
      },
      "cell_type": "markdown",
      "source": "### Missing Age\nAge is an important factor for survival prediction, since children are more likely to be saved. Instead of filling the NaN with general mean age, we may try to guess Passengers' ages according to their title."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "209a5a5751473625f2e25e037208385b929655f5"
      },
      "cell_type": "code",
      "source": "# Parse Titles from Names\ntitle = pd.DataFrame()\n\ndef parse_name(str):\n    m = re.search(', (\\w+ *\\w*)\\.',str)\n    return m.group(1)\n    \ntitle['Title'] = full.Name.map(parse_name)\ntitle.Title.unique()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8116b324ff891782bfdd58b0c532ccff7aeed5af"
      },
      "cell_type": "code",
      "source": "# Simplify title groups\nTitle_Dictionary = {\"Capt\":       \"Officer\",\n                    \"Col\":        \"Officer\",\n                    \"Major\":      \"Officer\",\n                    \"Jonkheer\":   \"Royalty\",\n                    \"Don\":        \"Royalty\",\n                    \"Sir\" :       \"Royalty\",\n                    \"Dr\":         \"Officer\",\n                    \"Rev\":        \"Officer\",\n                    \"the Countess\":\"Royalty\",\n                    \"Dona\":       \"Royalty\",\n                    \"Mme\":        \"Mrs\",\n                    \"Mlle\":       \"Miss\",\n                    \"Ms\":         \"Mrs\",\n                    \"Mr\" :        \"Mr\",\n                    \"Mrs\" :       \"Mrs\",\n                    \"Miss\" :      \"Miss\",\n                    \"Master\" :    \"Master\",\n                    \"Lady\" :      \"Royalty\"\n                    }\n\ntitle.Title = title.Title.map(Title_Dictionary)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b99af39b0000d48fd15cd95fd665d4f40abcc6f2"
      },
      "cell_type": "code",
      "source": "# Plot the distribution of Age by Title\ntitle = title.join(full.Age)\n# display(title.groupby('Title').describe());\nplt.figure(figsize = (14,6))\nsns.violinplot(data = title, x = 'Title', y = 'Age');",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fd27829a0be21defa1c6568578d8d7f6bc9a73f9"
      },
      "cell_type": "markdown",
      "source": "Passengers with title 'Master' are likely to be children, we can infer those missing age as the mean age of Master\nPassengers with title 'Miss' seem to comprise both children and adult, the followings is an attempt to infer their age from other given features <br>\nHowever, age of female here is relatively unimportant, since all female regardless of age have high priority to board the lifeboats."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "71f178e579b1d6c20d4141e6d7c203a2d541ba38"
      },
      "cell_type": "code",
      "source": "title_age_dict = {}\n\n# Calculate mean age of each title group\nfor t in title.Title.unique():\n    title_age_dict[t] = title[title.Title == t].Age.mean() \n\n# Fill in Age according to passenger's title\nidx = full.Age.isnull()\nfull['Age_infer'] = full['Age']\nfull.loc[idx, 'Age_infer'] = title.loc[idx].Title.map(title_age_dict)\nfull['Child'] = (full['Age_infer'] <= 14).astype(int)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f1d02d3ae8eee0f4d3ac606ea8c9812c5347ab6d"
      },
      "cell_type": "markdown",
      "source": "## Data Transformation\nUsed MinMaxScalar for continuous variables and One-hot encoding for Categorical ones."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "fd3765b2215c705a41193ea631c26fab5eb3e8b1"
      },
      "cell_type": "code",
      "source": "from sklearn.preprocessing import MinMaxScaler\n\n# Select features as predictors\nfeatures = pd.concat([full[['Pclass','Sex','Child','Fare_adj','Parch','SibSp']],\n                      group[['Group_size','Confidence_member_survived']],\n                      title['Title'],\n                      ticket[['A5','PC']]], axis = 1)\n\n# MinMax Transform the continuous variables\nscalar = MinMaxScaler()\n\ncontinuous = ['Fare_adj','Group_size']\nfeatures_minmax_transformed = pd.DataFrame(data = features)\nfeatures_minmax_transformed[continuous] = scalar.fit_transform(features_minmax_transformed[continuous])\n\n# Transform Sex labels into binary code\nfeatures_minmax_transformed.Sex = features_minmax_transformed.Sex.apply(lambda x: 1 if x == 'male' else 0)\n\n# One-hot Encoding\nfeatures_final = pd.get_dummies(features_minmax_transformed)\n\nencoded = list(features_final.columns)\nprint(\"{} total features after one-hot encoding.\".format(len(encoded)))\n\n# Seperate Train Data and Test Data\nfeatures_final_train = features_final[:891]\nfeatures_final_test = features_final[891:]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "01cfc65eded30aeb785046e01591d713ea441086"
      },
      "cell_type": "markdown",
      "source": "## Model Training and Selection\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "46541bd74d5e886853601434987b126c444797be"
      },
      "cell_type": "code",
      "source": "# Spliting Training Sets into Train and Cross-validation sets\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n\nX_train, X_test, y_train, y_test = train_test_split(features_final_train, \n                                                    train.Survived, \n                                                    test_size = 0.2, \n                                                    random_state = 0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a6e4f58b457d4094bb16b848f4212a69b0fb3910"
      },
      "cell_type": "code",
      "source": "# Create Model Training Pipeline\nfrom sklearn.metrics import accuracy_score\nfrom time import time\n\ndef train_predict(learner, sample_size, X_train, y_train, X_test, y_test): \n    '''\n    inputs:\n       - learner: the learning algorithm to be trained and predicted on\n       - sample_size: the size of samples (number) to be drawn from training set\n       - X_train: features training set\n       - y_train: income training set\n       - X_test: features testing set\n       - y_test: income testing set\n    '''\n    \n    results = {}\n    \n    # Fit the learner to the training data using slicing with 'sample_size' using .fit(training_features[:], training_labels[:])\n    learner = learner.fit(X_train[:sample_size], y_train[:sample_size])\n    \n    # Get the predictions on the test set(X_test),\n    predictions_test = learner.predict(X_test)\n    \n    # then get predictions on the training samples(X_train)\n    predictions_train = learner.predict(X_train)\n            \n    # Compute accuracy on the training samples\n    results['acc_train'] = accuracy_score(y_train, predictions_train)\n        \n    # Compute accuracy on test set using accuracy_score()\n    results['acc_test'] = accuracy_score(y_test, predictions_test)\n       \n    # Success\n    print(\"{} trained on {} samples. Acc: {:.4f}\".format(learner.__class__.__name__, sample_size, results['acc_test']))\n        \n    # Return the results\n    return results",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "7515c55d4c3d55d39d50f5ae0fcff497b627a86f"
      },
      "cell_type": "code",
      "source": "# Import the three supervised learning models from sklearn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier,RandomForestClassifier\n\n# Initialize the three models\nclf_A = GradientBoostingClassifier(random_state = 0)\nclf_B = LogisticRegression(random_state= 0)\nclf_C = RandomForestClassifier(random_state= 0)\n\n# Calculate the number of samples for 10%, 50%, and 100% of the training data\nsamples_100 = len(y_train)\nsamples_10 = int(len(y_train)/2)\nsamples_1 = int(len(y_train)/10)\n\n# Collect results on the learners\nresults = {}\nfor clf in [clf_A, clf_B, clf_C]:\n    clf_name = clf.__class__.__name__\n    results[clf_name] = {}\n    for i, samples in enumerate([samples_1, samples_10, samples_100]):\n        results[clf_name][i] = \\\n        train_predict(clf, samples, X_train, y_train, X_test, y_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "024d7f953b517522e4299275a569ad08457b921b"
      },
      "cell_type": "code",
      "source": "# Reshaping the Results for plotting\ndf = pd.DataFrame()\n\nfor i in results.items():\n    temp = pd.DataFrame(i[1]).rename(columns={0:'1% of train', 1:'10% of train', 2:'100% of train'})\n    temp['model'] = i[0]\n    df = pd.concat([df, temp], axis = 0)\ndf_plot = df.reset_index().melt(id_vars=['index','model'])\n\n# Ploting the results\nfig, axs = plt.subplots(1,2,figsize = (14,4))\nfor i,key in enumerate(df_plot['index'].unique()[:2]):\n    ax = axs[i%2]\n    sns.barplot(data = df_plot[df_plot['index'] == key], x = 'model', y = 'value',\n                hue = 'variable', ax = ax)\n    ax.set_ylim([0,1])\n    ax.set_title(key)\n    ax.legend(ncol=3, loc=\"lower right\", frameon=True, fontsize = 'small')\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1f642f0d6f052b42b12ec340fb0e0682c50fca74"
      },
      "cell_type": "markdown",
      "source": "## Model Selection and model tuning\nRandomForestClassifier seemed to have the best out of the box accuracy score and with room for improvement as seen in acc_train.\nModel tuning is performed using GridSearchCV to improve generalizability of the model."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "50dad97e49723373414d910c553cf71de0f43681"
      },
      "cell_type": "code",
      "source": "from sklearn.grid_search import GridSearchCV\nfrom sklearn.metrics import make_scorer\n\nclf = RandomForestClassifier(random_state = 0, oob_score = True)\n\nparameters = {'criterion' :['gini'],\n             'n_estimators' : [400], #[100,200,400]\n             'max_depth':[6], #[3,4,5,6]\n             'min_samples_leaf': [5], # [2,4,6]\n              'max_leaf_nodes': [10], # [8,10,12]\n              'min_impurity_decrease': [0], # [0,0.001,0.005]\n              'max_features' : [1] # [1,2,3]\n             }\n\nscorer = make_scorer(accuracy_score)\n\ngrid_obj = GridSearchCV(clf, parameters, scoring = scorer, cv = 10)\n\ngrid_fit = grid_obj.fit(X_train,y_train)\n\nbest_clf = grid_fit.best_estimator_\n\npredictions = (clf.fit(X_train, y_train)).predict(X_test)\nbest_predictions = best_clf.predict(X_test)\n\nprint(\"Unoptimized model\\n------\")\nprint(\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_test, predictions)))\nprint(\"Oob score on testing data: {:.4f}\".format(clf.oob_score_))\nprint(\"\\nOptimized Model\\n------\")\nprint(\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\nprint(\"Final oob score on the testing data: {:.4f}\".format(best_clf.oob_score_))\nprint(\"\\nBest Parameters\\n------\")\nbest_clf",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6255c6c0cc86885259ff329d6bc56b92710afefc"
      },
      "cell_type": "code",
      "source": "# Plot Feature Importnace\nidx = np.argsort(best_clf.feature_importances_)\nplt.figure(figsize = (12,8))\nplt.barh(range(len(best_clf.feature_importances_)),best_clf.feature_importances_[idx])\nplt.yticks(range(len(best_clf.feature_importances_)),features_final_train.columns[idx]);\nplt.title('Feature Importance');",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4222258b969f99892223eff198fc96fac98c2ec7"
      },
      "cell_type": "code",
      "source": "# Output for Kaggle competition\nfinal_predict = best_clf.predict(features_final_test)\n\nprediction = pd.DataFrame(full[891:].PassengerId)\nprediction['Survived'] = final_predict.astype('int')\n\nprediction.to_csv('predict.csv',index = False)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}